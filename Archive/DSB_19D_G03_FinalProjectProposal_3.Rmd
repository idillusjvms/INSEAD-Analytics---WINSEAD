---
title: "Proposal for Final Project - Red Wine Quality"
author: "G03: Carolin, Yasmin, Chelsea, Jacobo, Yuki, Alejandro"
output:
  html_document:
    css: ../../AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../../AnalyticsStyles/default.sty
always_allow_html: yes
---
![Vinho Verde wine quality](Images\vinho-verde.jpg)

<!-- **Note:** Assuming the working directory is "MYDIRECTORY/INSEADAnalytics" (where you have cloned the course material), you can create an html file by running in your console the command (pay attention to the directory you are in, check it using getwd())

rmarkdown::render("CourseSessions/InClassProcess/MarketSegmentationProcessInClassParts1and2.Rmd") 
-->

```{r setuplibraries, echo=FALSE}
if("pacman" %in% rownames(installed.packages()) == FALSE) {install.packages("pacman")} # Check if you have universal installer package, install if not
pacman::p_load("corrplot","tidyr","ggplot2","plotly","viridis","forecast","caret","ROCR","lift","glmnet","MASS","e1071","outliers","nnet","foreign","stargazer","randomForest", "party") #Check, and if needed install the necessary packages

local_directory = "."
source(paste(local_directory,"/Library/functions_library.R", sep="/"))

```
```{r setupdata1E, echo=TRUE, tidy=TRUE}
#CHOOSE FILE TO ANALYSE: Please use "Red_Wine.csv"
redwine<-read.csv('Red_Wine.csv', header=TRUE, sep=",")
```

<hr>\clearpage

# A. The Business Problem

<img src="Images\Portugal_vinho_verde.jpg" width="200px" align="left" hspace="20"/>

Our winery in the Northwest of Portugal produces Vinho Verde wine. More recently we have seen the quality of our red wine decline, each year losing spots at the internationally acclaimed “Best of Vinho Verde awards”. 

We are wondering if there is anything we could do to improve the quality and rating of the wine by impacting the production process and influence some physio-chemical characteristics of the wine. Our rational is that the grapes are the same as our competitors so there is certainly room for improvement. We have a limited budget to work on the improvement of our wine and would like to understand which components matter most and how to invest to improve the production of better wine. 
<br clear="left"/>

We gathered data from 1599 red Vinho Verde wines with their measurable characteristics: 

1.	**Fixed acidity (tartaric acid - g / dm^3^)**: most acids involved with wine or fixed or nonvolatile (do not evaporate readily).
2.	Volatile acidity (acetic acid - g / dm^3^)**: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.
3.	**Citric acid (g / dm^3^)**: found in small quantities, citric acid can add 'freshness' and flavor to wines
4.	**Residual sugar (g / dm^3^)**: the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet.
5.	**Chlorides (sodium chloride - g / dm^3^)**: the amount of salt in the wine.
6.	**Free sulfur dioxide (mg / dm^3^)**: the free form of SO~2~ exists in equilibrium between molecular SO~2~ (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine.
7.	**Total sulfur dioxide (mg / dm^3^)**: amount of free and bound forms of S0~2~; in low concentrations, SO~2~ is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO~2~ becomes evident in the nose and taste of wine
8.	**Density (g / cm^3^)**: the density of water is close to that of water depending on the percent alcohol and sugar content
9.	**pH**: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale
10.	**Sulphates (potassium sulphate - g / dm^3^)**: a wine additive which can contribute to sulfur dioxide gas (S0~2~) levels, which acts as an antimicrobial and antioxidant
11.	**Alcohol (% by volume)**: the percent alcohol content of the wine
12. **Quality**: The expert quality rating has been calculated as the median of 3 evaluations made by experts purely based on their sensorial experience of tasting the wines.


<hr>\clearpage

## A.1. The Summary Statistics Table for Our Dataset

The basic structure of the data is as follows:

```{r, echo=FALSE,tidy=TRUE}
#creating summary stats
str(redwine)
```

A summary of the data can be found below:

```{r, echo=FALSE,tidy=TRUE}
#summary(redwine)
knitr::kable(summary(redwine),font_size = 7,align = "c")
```

The correlation of the data is shown in the graph below

```{r, echo=FALSE}
Corr_redwine<-cor(redwine)
###Visualization of the correlations
corrplot(Corr_redwine, method = "circle") #plot matrix
```

<hr>\clearpage

## A.2. The Business Solution Process 

We are thus exploring whether there is a link between purely physio-chemical characteristics and perceived quality of the wines in order to tailor our production process to consumer preferences in order to achieve consistent quality of our wines. To conduct this analysis, we plan to execute the following steps: 

1.  Descriptive statistics of the data (preview summary above) 
2.  Review the data for outliers, clean if needed, and create additional factors or variables as we see fit and select the variables that will be relevant for the model 
3.  Split the data into a training and a test data set 
4.  Conduct regression analysis on the training data set 
5.  Explore value of further analysis (e.g., factor analysis and segmentation) 
6.  Develop different models to predict the best wine 
7.  Evaluate our different models using the test data set 
8.  Select the model with the less errors in the predictions 

Once we have the best model, we will be able to know the characteristics that make a great wine. We will be able to adapt our production process in order to produce a wine that will win awards, use our investment resources in the most effective way and that will appeal to the customers. 

<hr>\clearpage

# 0. Understanding your data

Understanding out wine data is one of the major activities during the wine data analysis. Understanding our wine data deals with detecting and removing errors and inconsistencies from the wine data in order to improve the quality of data. It will also play major role during decision-making process.

A good approach should satisfy several requirements. First of all, we have a dictionary where all the variables are explained. Then, we should detect and remove all major errors and inconsistencies from the wine data. This approach is supported by R tools to limit manual inspection and programming effort.

<hr>\clearpage

# 1 Descriptive statistics of the data 

## 1.1 Data format checking:

The first step is to ensure that the data read from the CSV file is read in the right format. 

```{r}
redwine$fixed.acidity <- as.numeric(redwine$fixed.acidity)
redwine$volatile.acidity <- as.numeric(redwine$volatile.acidity)
redwine$citric.acid <- as.numeric(redwine$citric.acid)
redwine$residual.sugar <- as.numeric(redwine$residual.sugar)
redwine$chlorides <- as.numeric(redwine$chlorides)
redwine$free.sulfur.dioxide <- as.numeric(redwine$free.sulfur.dioxide)
redwine$total.sulfur.dioxide <- as.numeric(redwine$total.sulfur.dioxide)
redwine$density <- as.numeric(redwine$density)
redwine$pH <- as.numeric(redwine$pH)
redwine$sulphates <- as.numeric(redwine$sulphates)
redwine$alcohol <- as.numeric(redwine$alcohol)
redwine$quality <- as.integer(redwine$quality)
```

The data is now in the right format, that's fantastic! 

## 1.2. Summary Statistics

The wine data looks like this:

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
knitr::kable({
  df <- t(head(round(redwine[,],2), 10))
  colnames(df) <- sprintf("Wine %2d", 1:ncol(df))
  df
})
```


The basic structure of the data is as follows:

```{r, echo=FALSE,tidy=TRUE}
#creating summary stats
str(redwine)
```

A summary for each variable can be found below:

```{r, echo=FALSE,tidy=TRUE}
#summary(redwine)
knitr::kable(summary(redwine),font_size = 5,align = "c")
```

<hr>\clearpage

# 2. Review data and select variables

## 2.1 Data inconsistency checking

The second step is to ensure that the data read from the CSV file does not contain any missing values. As shown in the summary data above, it does not contain any missing values. Thus, we can proceed with the study. 

## 2.2 Histograms

We plot the histograms of the data provided:

```{r, echo=FALSE}
# Density plot
colnames <- dimnames(redwine)[[2]]

par(mfrow=c(1, 3))
for (i in 1:3) {
  x_string=colnames[i]
  hist_den(redwine, x_string, paste("Histogram for\n",x_string))
}

par(mfrow=c(1, 3))
for (i in 4:6) {
  x_string=colnames[i]
  hist_den(redwine, x_string, paste("Histogram for\n",x_string))
}

par(mfrow=c(1, 3))
for (i in 7:9) {
  x_string=colnames[i]
  hist_den(redwine, x_string, paste("Histogram for\n",x_string))
}

par(mfrow=c(1, 3))
for (i in 10:12) {
  x_string=colnames[i]
  hist_den(redwine, x_string, paste("Histogram for ",x_string))
}

```

## 2.3 Data correlation

Now, we will see in more detail the correlation of each variable. It will be interesting for the project. The correlation matrix is:

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
Corr_redwine<-cor(redwine)
#round(Corr_redwine,digits = 2) # We limit the number of digits when showing the results

knitr::kable({
  df <- t(head(round(Corr_redwine[,],2), 12))
  #colnames(df) <- sprintf("%2d", 1:ncol(df))
  df
})
```

And the graphical representation is:

```{r,echo=FALSE, tidy=TRUE}
###Visualization of the correlations
corrplot(Corr_redwine, method = "circle") #plot matrix
```

From the matrix and plot above, we can derive that:

* **Fixed Acidity**
  + There is a positive correlation with citric acid. This is true since citric acid is one of the fixed acid in wine.   
  + There is a positive correlation with density.
  + There is a significant negative correlation with pH.
* **Volatile Acidity**
  + There is a highly negatively correlation with citric acid
* **Free SO~2~**
  + There is a significant positive correlation with total SO~2~.
* **Density**
  + There is a significant negative correlation with alcohol and pH.
  + There is a positive correlation with  fixed acidity, citric acid and residual sugar
* **Quality (dependent variable)**
  + Quality and alcohol are positively correlated
  + Quality is negative correlated with volatile acidity.

## 2.4 Plots
### 2.4.1 Quality vs. other independent variables

And now we plot the rest of the variables against the quality:

```{r, echo=FALSE}

par(mfrow=c(1, 3))
plot(redwine$quality,redwine$fixed.acidity); title(main = "Fixed Acidity vs Quality");grid()
plot(redwine$quality,redwine$volatile.acidity); title(main = "Volatile Acidity vs Quality");grid()
plot(redwine$quality,redwine$citric.acid); title(main = "Citric Acid vs Quality");grid()

par(mfrow=c(1, 3))
plot(redwine$quality,redwine$residual.sugar); title(main = "Sugar vs Quality");grid()
plot(redwine$quality,redwine$chlorides); title(main = "Chlorides vs Quality");grid()
plot(redwine$quality,redwine$free.sulfur.dioxide); title(main = "Free SO2 vs Quality");grid()

par(mfrow=c(1, 3))
plot(redwine$quality,redwine$total.sulfur.dioxide); title(main = "Total SO2 vs Quality");grid()
plot(redwine$quality,redwine$density); title(main = "Density vs Quality");grid()
plot(redwine$quality,redwine$pH); title(main = "PH vs Quality");grid()

par(mfrow=c(1, 2))
plot(redwine$quality,redwine$sulphates); title(main = "Sulphates vs Quality");grid()
plot(redwine$quality,redwine$alcohol); title(main = "Alcohol vs Quality");grid()

```

### 2.4.2 Additional plots

In this section, we plot other graphs to see how the data is distributed based on the correlation matrix 

```{r echo = FALSE, Density_VS_alcohol}

### One plot

p<- ggplot(redwine, aes(x = density, y = alcohol, color = quality)) + geom_jitter(alpha = 1/2) + 
  ggtitle("Density VS Alcohol") + xlab("Density") + ylab("Alcohol") + labs(color = "Quality") + 
  scale_color_gradientn(colours = rainbow(5))

mytext = paste("Density: ",redwine$density,"\n",
               "Alcohol = ", redwine$alcohol, "\n" , 
               "Quality = ", redwine$quality, "\n" ,
               sep="")    
pp<- plotly_build(p)   # here note that p was already created with ggplot
style(pp,text=mytext, hoverinfo = "text") %>% layout(showlegend = FALSE)

### ANother plot

p<- ggplot(redwine, aes(x = fixed.acidity, y = pH, color = quality)) + geom_jitter(alpha = 1/2) + 
  ggtitle("Fixed Acidity VS pH") + xlab("Fixed acidity") + ylab("pH") + labs(color = "Quality") + 
  scale_color_gradientn(colours = rainbow(5))

mytext = paste("Fixed Acidity: ",redwine$fixed.acidity,"\n",
               "pH = ", redwine$pH, "\n" , 
               "Quality = ", redwine$quality, "\n" ,
               sep="")    
pp<- plotly_build(p)   # here note that p was already created with ggplot
style(pp,text=mytext, hoverinfo = "text") %>% layout(showlegend = FALSE)

### ANother plot

p<- ggplot(redwine, aes(x = fixed.acidity, y = density, color = quality)) + geom_jitter(alpha = 1/2) + 
  ggtitle("Fixed Acidity VS Density") + xlab("Fixed acidity") + ylab("Density") + labs(color = "Quality") + 
  scale_color_gradientn(colours = rainbow(5))

mytext = paste("Fixed Acidity: ",redwine$fixed.acidity,"\n",
               "Density = ", redwine$density, "\n" , 
               "Quality = ", redwine$quality, "\n" ,
               sep="")    
pp<- plotly_build(p)   # here note that p was already created with ggplot
style(pp,text=mytext, hoverinfo = "text") %>% layout(showlegend = FALSE)

### ANother plot

p<- ggplot(redwine, aes(x = volatile.acidity, y = citric.acid, color = quality)) + geom_jitter(alpha = 1/2) + 
  ggtitle("Volatile Acidity VS Citric Acid") + xlab("Volatile acidity") + ylab("Citric Acid") + labs(color = "Quality") + 
  scale_color_gradientn(colours = rainbow(5))

mytext = paste("Volatile Acidity: ",redwine$volatile.acidity,"\n",
               "Citric Acid = ", redwine$citric.acid, "\n" , 
               "Quality = ", redwine$quality, "\n" ,
               sep="")    
pp<- plotly_build(p)   # here note that p was already created with ggplot
style(pp,text=mytext, hoverinfo = "text") %>% layout(showlegend = FALSE)


### ANother plot

p<- ggplot(redwine, aes(x = free.sulfur.dioxide, y = total.sulfur.dioxide, color = quality)) + geom_jitter(alpha = 1/2) + 
  ggtitle("Free SO2 VS Total SO2") + xlab("Free SO2") + ylab("Total SO2") + labs(color = "Quality") + 
  scale_color_gradientn(colours = rainbow(5))

mytext = paste("Free SO2: ",redwine$free.sulfur.dioxide,"\n",
               "Total SO2 = ", redwine$total.sulfur.dioxide, "\n" , 
               "Quality = ", redwine$quality, "\n" ,
               sep="")    
pp<- plotly_build(p)   # here note that p was already created with ggplot
style(pp,text=mytext, hoverinfo = "text") %>% layout(showlegend = FALSE)

### ANother plot

p<- ggplot(redwine, aes(x = alcohol, y = citric.acid, color = quality)) + geom_jitter(alpha = 1/2) + 
  ggtitle("Alcohol VS Citric Acid") + xlab("Alcohol") + ylab("Citric Acid") + labs(color = "Quality") + 
  scale_color_gradientn(colours = rainbow(5))

mytext = paste("Alcohol: ",redwine$alcohol,"\n",
               "Citric Acid = ", redwine$citric.acid, "\n" , 
               "Quality = ", redwine$quality, "\n" ,
               sep="")    
pp<- plotly_build(p)   # here note that p was already created with ggplot
style(pp,text=mytext, hoverinfo = "text") %>% layout(showlegend = FALSE)

```

## 2.5 Selection of variables

The main objective of our exercise is to help identify poor quality wine based on its chemical attributes. Poor quality, or faulty wines, have been defined in our dataset based on their quality:

```{r echo=FALSE}
hist_den(redwine, colnames[12], "Distribution of wine quality")
```

Wines with a quality below 5 are considered **bad wines** - we **do not want to sell** these wines to the public. This category is of primary interest in our study.

In addition to this, there is a clear distinction between the number of wines classified with quality 5 & 6 and between the number of wines classified as over 6. Thus, a good way of classification is as follows: **average wines** are those wines whose quality is between **5 and 6** and **good wines** are those whose quality is above 6.

```{r echo=FALSE, message=FALSE, prompt=FALSE}
redwine <- cbind(redwine, ifelse(redwine[,"quality"] < 5, 'faulty',
                                         ifelse(redwine[,"quality"] == 6 | redwine[,"quality"] == 5, 'average', 
                                                'good')))
colnames(redwine)[13] <- "taste"
```

Classification | Quality | # Occurrences in estimation data
:------|:-----------|:----------|:-----------|:----------
Faulty  | 4 or less | `r sum(redwine[,"taste"] == 'faulty')` (`r round(sum(redwine[,"taste"] == 'faulty')/nrow(redwine),2)*100`%)
Average  | 5 & 6 | `r sum(redwine[,"taste"] == 'average')` (`r round(sum(redwine[,"taste"] == 'average')/nrow(redwine),2)*100`%) |
Good | 7 or greater | `r sum(redwine[,"taste"] == 'good')` (`r round(sum(redwine[,"taste"] == 'good')/nrow(redwine),2)*100`%)
Total | - | `r sum(redwine[,"taste"] == 'good')+sum(redwine[,"taste"] == 'average')+sum(redwine[,"taste"] == 'faulty')`


### Faulty wine

```{r echo = FALSE}
knitr::kable(round(my_summary(redwine[redwine[,"taste"] == 'faulty',1:12]),2))
```

### Normal wine

```{r echo = FALSE}
knitr::kable(round(my_summary(redwine[redwine[,"taste"] == 'average',1:12]),2))
```

### Good wine

```{r echo = FALSE}
knitr::kable(round(my_summary(redwine[redwine[,"taste"] == 'good',1:12]),2))
```

<hr>\clearpage

# 3. Split the data into a training and a test data set


```{r}
set.seed(1985) #set a random number generation seed to ensure that the split is the same everytime

redwine_split <- createDataPartition(y =,redwine$quality,
                               p = 1298/1599, list = FALSE) # we put one less to make the training. We use 80% to estimate the  value and 20 percent for training

training_redwine <- redwine[ redwine_split,]
testing_redwine<- redwine[ -redwine_split,]
```

<hr>\clearpage

# 4. Analysis process

The analysis process carried out is based on the 6-step process provided in class. We decided that we would use an implementation of Breiman and Cutler’s Random Forests for Classification and Regression. As a result we did not have to restrict ourselves to a binary dependent variable.


## 4.1 Multinomial logistic regression
### 4.1.1 Simple multinomial logistic regression

The first test we are going to do is the multinomial logistic regression. The idea is simple, we will try to derive the taste quality parameter based on the other 12 independent variables. A summary of the regression is shown below

```{r, echo=FALSE, message=FALSE, prompt=FALSE, results='hide'}

model_mr_wine <- multinom(taste ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar +
                            chlorides + free.sulfur.dioxide + total.sulfur.dioxide + 
                            density + pH + sulphates + alcohol, data = training_redwine)
```

```{r, echo=FALSE, message=FALSE, prompt=FALSE}
summary(model_mr_wine)
knitr::kable(summary(model_mr_wine)$coefficients,font_size = 7,align = "c",caption="Coefficients for Simple multinomial regression")
```

As multinomial logistic regression does not provide the p-values, we will calculate them by normalizing the results. The calculated p-values are as follows:

```{r echo=FALSE, message=FALSE, prompt=FALSE}
z <- summary(model_mr_wine)$coefficients/summary(model_mr_wine)$standard.errors
p <- (1-pnorm(abs(z), 0, 1)) * 2
knitr::kable(p,font_size = 7,align = "c",caption="P-value table for Simple multinomial regression")
```

The model summary output has a block of coefficients and a block of standard errors. Each of these blocks has one row of values corresponding to a model equation. Focusing on the block of coefficients, we can look at the first row comparing prog = “faulty” to our baseline prog = “average” and the second row comparing prog = “good” to our baseline prog = “average”. If we consider our coefficients from the first row to be b1 and our coefficients from the second row to be b2, we can write our model equations as follows


$$\ln(\dfrac{P(prog=faulty)}{P(prob=average)})=b_{10}+b_{11}+b_{12}+b_{13}$$
$$\ln(\dfrac{P(prog=good)}{P(prob=average)})=b_{20}+b_{21}+b_{22}+b_{23}$$
We can also use predicted probabilities to help you understand the model. We can calculate predicted probabilities for each of our outcome levels using the fitted function. We can start by generating the predicted probabilities for the observations in our dataset and viewing the first few rows

```{r, echo=FALSE}
pp <- fitted(model_mr_wine)
pp <- cbind(pp, training_redwine$taste)
colnames(pp)[4] <- "Taste from original data (1 is average, 2 is faulty and 3 is good)"
knitr::kable( pp[1:10,],font_size = 7,align = "c")

```

We can predict the test values based on this regression:
```{r, echo=FALSE}
MR_prob<-predict(model_mr_wine,testing_redwine,"probs")

predicted_class <- predict(model_mr_wine, testing_redwine)

knitr::kable(table(predicted_class,testing_redwine$taste),font_size = 7,align = "c")

missclasificacion_simple <- mean(as.character(predicted_class) != as.character(testing_redwine$taste))*100

#sprintf("We have an missclassification error of %.2f %%", mean(as.character(predicted_class) != as.character(testing_redwine$taste))*100)
        
# #MR_ROC_prediction <- prediction(MR_prob, testing_redwine$taste)
# MR_logistic_ROC <- performance(MR_ROC_prediction,"tpr","fpr") #Create ROC curve data
# par(mfrow=c(1,1))
# plot(MR_logistic_ROC) #Plot ROC curve
```

Based on the outputs, we have an missclassification error of `r missclasificacion_simple` %.  Also, we can plot the ROC curve. The ROC curve illustrates the performance of a binary classifier system as its discrimination threshold varies. The curve shows the true positive rate against the false positive rate at various threshold settings. The true-positive rate is also known as recall and the false-positive rate is also known as the fall-out or probability of false alarm. 

```{r echo = FALSE}
vd_simple <- cbind(training_redwine, ifelse(training_redwine[,"taste"] == "good", 'ok',
                                         ifelse(testing_redwine[,"quality"] == "average", 'ok', 'not ok')))
colnames(vd_simple)[14] <- "taste2"

OOB.votes <- predict(model_mr_wine, vd_simple, type = "prob")
OOB.pred  <- OOB.votes[,3]
pred.obj  <- prediction(OOB.pred, vd_simple[,"taste2"])
ROC.perf  <- ROCR::performance(pred.obj, "tpr", "fpr")
plot(ROC.perf)
```

```{r echo=FALSE}
MR_wine.tmp <- performance(pred.obj,"auc") #Create AUC data
MR_wine_auc_testing <- as.numeric(MR_wine.tmp@y.values) #Calculate AUC
#Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```
The area under the curve is `r MR_wine_auc_testing*100`%. 

***Note***: Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value.

### 4.1.2 Stepwise multinomial logistic regression

In this case, we use stepwise regression to find the best parameters. The summary of the simulation is as follows

```{r, echo=FALSE, message=FALSE, results='hide'}
model_multy_stepwise_both<-stepAIC(model_mr_wine,direction = c("both"),trace = 1) #AIC stepwise
#summary(model_multy_stepwise_both)  #tepwise is nt the best model but we can try
```
```{r, echo = FALSE}
knitr::kable(summary(model_multy_stepwise_both)$coefficients,font_size = 7,align = "c",caption="Coefficients table for Stepwise multinomial regression")
#summary(model_multy_stepwise_both)  #tepwise is nt the best model but we can try
```

Again, we need to calculate the new p-values based on the previous results

```{r, echo=FALSE}
z <- summary(model_multy_stepwise_both)$coefficients/summary(model_multy_stepwise_both)$standard.errors
p <- (1-pnorm(abs(z), 0, 1)) * 2
knitr::kable(p,font_size = 7,align = "c",caption="P-value table for Stepwise multinomial regression")
```

We can predict the test values based on the stepwise multimodal regression:
```{r, echo=FALSE}
MR_prob<-predict(model_multy_stepwise_both,testing_redwine,"probs")

predicted_class <- predict(model_multy_stepwise_both, testing_redwine)

knitr::kable(table(predicted_class,testing_redwine$taste),font_size = 7,align = "c")

missclasificacion_step <- mean(as.character(predicted_class) != as.character(testing_redwine$taste))*100

#sprintf("We have an missclassification error of %.2f %%", mean(as.character(predicted_class) != as.character(testing_redwine$taste))*100)
        
# #MR_ROC_prediction <- prediction(MR_prob, testing_redwine$taste)
# MR_logistic_ROC <- performance(MR_ROC_prediction,"tpr","fpr") #Create ROC curve data
# par(mfrow=c(1,1))
# plot(MR_logistic_ROC) #Plot ROC curve
```

Based on the outputs, we have an missclassification error of `r missclasificacion_step` %. Also, we can plot the ROC curve. The ROC curve illustrates the performance of a binary classifier system as its discrimination threshold varies. The curve shows the true positive rate against the false positive rate at various threshold settings. The true-positive rate is also known as recall and the false-positive rate is also known as the fall-out or probability of false alarm. 

```{r echo = FALSE}
vd_step <- cbind(training_redwine, ifelse(training_redwine[,"taste"] == "good", 'ok',
                                         ifelse(testing_redwine[,"quality"] == "average", 'ok', 'not ok')))
colnames(vd_step)[14] <- "taste2"

OOB.votes <- predict(model_multy_stepwise_both, vd_step, type = "prob")
OOB.pred  <- OOB.votes[,3]
pred.obj  <- prediction(OOB.pred, vd_step[,"taste2"])
ROC.perf  <- ROCR::performance(pred.obj, "tpr", "fpr")
plot(ROC.perf)
```

```{r echo=FALSE}
AIC_wine.tmp <- performance(pred.obj,"auc") #Create AUC data
AIC_wine_auc_testing <- as.numeric(AIC_wine.tmp@y.values) #Calculate AUC
#Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```
The area under the curve is `r AIC_wine_auc_testing*100`%. 

***Note***: Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value.


### 4.1.3 Summary of the results

With the simple multinomial regression taking into account all the variables, we are able to predict the quality of wine with a missclassification error of `r missclasificacion_simple` %. Based on the p-values, the parameters that characterize a good wine are as follows:

* **fixed acidity**: Increase fixed acidity
* **Volatile acidity**: Decrese volatile acidity
* **Residual Sugar**: Increase residual sugar
* **Density**: Reduce the density
* **Chlorides**: Reduce cholrides
* **Total SO2**: Reduce total SO2
* **Sulphates**: Increase the sulphates
* **Alcohol**: Increase the alcohol

Also, based on the results, the key differences of a good wine from a bad wine are found in the **Total SO2**, **Sulphates** and the **Alcohol** level. We will need topay attention to those values. 


## 4.2 Classification and Interpretation: Random Forest tree
### 4.2.1 Refining the parameters

It is time now to run a classification algorithm on the data set. We have chosen to use the random forest tree algorithm for this.

```{r echo = FALSE}
my.sampsize = c(sum(training_redwine[,"taste"]=="faulty"), sum(training_redwine[,"taste"]=="faulty"), sum(training_redwine[,"taste"]=="faulty")) #maximum unbiased sample size
my.cutoff = c(0.4,0.3,0.3)
#my.cutoff = c(0.6,0.15,0.25)
my.classwt = c(1e-05,1,1)
my.mtry = 9
my.ntree = 256

Wine_tree <- randomForest(taste ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + 
                            chlorides + free.sulfur.dioxide + total.sulfur.dioxide + 
                            density + pH + sulphates + alcohol, data = training_redwine, ntree=my.ntree, mtry=my.mtry, sampsize=my.sampsize, cutoff = my.cutoff, classwt = my.classwt, importance = TRUE)

Wine_tree_base <- randomForest(taste ~ . - quality, data = training_redwine)
```

First of all, we have a look at the confusion matrix results:

```{r echo = FALSE}
confusionMatrix <- Wine_tree$confusion
rownames(confusionMatrix) <- c("Actual average", "Actual faulty", "Actual good")
colnames(confusionMatrix) <- c("Predicted average", "Predicted faulty", "Predicted good", "Class error")
knitr::kable(confusionMatrix, digits = 2) #predicted in columns, actual in rows
```

And now the same confusion matrix from a percentage in class perspective (rows sum too 100%)

```{r echo = FALSE}

confusionMatrix[1,1:3] <- paste(round(confusionMatrix[1,1:3]/sum(as.numeric(confusionMatrix[1,1:3]))*100,2),"%", sep = "")
confusionMatrix[2,1:3] <- paste(round(as.numeric(confusionMatrix[2,1:3])/sum(as.numeric(confusionMatrix[2,1:3]))*100,2),"%", sep = "")
confusionMatrix[3,1:3] <- paste(round(as.numeric(confusionMatrix[3,1:3])/sum(as.numeric(confusionMatrix[3,1:3]))*100,2),"%", sep = "")
confusionMatrix[,4] <- paste(round(as.numeric(confusionMatrix[,4])*100,2),"%", sep = "")
knitr::kable(confusionMatrix[,1:3], digits = 2) 
```

This is how the error looks like:
```{r echo = FALSE}
plot(Wine_tree, main="Error rates for number of RandomForest trees")
```

After several trials, it seems that the error tends toget stabilized after the 80 trees. We have selected 128 trees.  

For the final version of the model, we tried several combinations until we found the ones we like:

Parameter | Value (average, faulty, good)
:---------|:-----------
classwt  | `r my.classwt`
sampsize  | `r my.sampsize`
cutoff  | `r my.cutoff`
mtry | `r my.mtry`
ntree | `r my.ntree`

### 4.2.2 Prediction based on the parameters

After running the predictionw with the testing sample, the following confusion matrix is obtained:

```{r echo=FALSE}
Wine_pred <- randomForest:::predict.randomForest(Wine_tree, testing_redwine)
Wine_base.pred <- randomForest:::predict.randomForest(Wine_tree_base, testing_redwine)
confusion.matrix <- confusionMatrix(reference = testing_redwine[, "taste"], data = Wine_pred)
base.confusion.matrix <- confusionMatrix(reference = testing_redwine[, "taste"], data = Wine_base.pred)
confusion.matrix
```

The ROC curve illustrates is as follows.

```{r echo = FALSE}
vd <- cbind(training_redwine, ifelse(training_redwine[,"taste"] == "good", 'ok',
                                         ifelse(testing_redwine[,"quality"] == "average", 'ok', 'not ok')))
colnames(vd)[14] <- "taste2"

OOB.votes <- predict(Wine_tree, vd, type = "prob")
OOB.pred  <- OOB.votes[,3]
pred.obj  <- prediction(OOB.pred, vd[,"taste2"])
ROC.perf  <- ROCR::performance(pred.obj, "tpr", "fpr")
plot(ROC.perf)
```

```{r echo=FALSE}
Tree_wine.tmp <- performance(pred.obj,"auc") #Create AUC data
Tree_wine_auc_testing <- as.numeric(Tree_wine.tmp@y.values) #Calculate AUC
#Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```
The area under the curve is `r Tree_wine_auc_testing*100`%. 

***Note***: Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value.

### 4.2.3 Summary of the random forest

The model is strongly **biased towards** the **faulty classification**. The model is pretty good at predicting the good wines, and this is exacly what we are looking for. Although the model might seem to be terrible at dealing with the average wines (usually splitting them 47% average, 24% faulty, 28% good) we are not concerned with upward misclassifications. Also, with such an abundance of average wines, losing out on 47% of them is not so terrible. The main objective is to select the best wines to increase our revenues.

The variable importance is as follows:

```{r echo = FALSE}
knitr::kable(importance(Wine_tree),font_size = 7,align = "c")
```
And with the graphical representation

```{r echo = FALSE}

varImpPlot(Wine_tree, sort = T, main = "Variable Importance", n.var = 11)
```

Based on the importance, the parameters that characterize a good wine are as follows:

* **Alcohol**: Increase the alcohol
* **Total SO2**: Reduce total SO2
* **Sulphates**: Increase the sulphates
* **Residual Sugar**: Increase residual sugar
* **Density**: Reduce the density
* **Volatile acidity**: Decrese volatile acidity

<hr>\clearpage

# 5. Summary and conclusions

We have run two different methods to find out the best variable predictors for our wine: **Multilinear regression** and **Ramdon Forest**. The outcome of the study is more or less similar.

On the one hand, the ** Multilinear regressesion** model is pretty good as we get a missclassification error of `r missclasificacion_simple` %. This means that we are able to analyze the wines pretty well. Simulationr resuls also shown that these are the main important parameters to be considered when doing a good wine: 

* **fixed acidity**: Increase fixed acidity
* **Volatile acidity**: Decrese volatile acidity
* **Residual Sugar**: Increase residual sugar
* **Density**: Reduce the density
* **Chlorides**: Reduce cholrides
* **Total SO2**: Reduce total SO2
* **Sulphates**: Increase the sulphates
* **Alcohol**: Increase the alcohol

And we do not need to forget about the **key differences** of a **good wine** from a **bad wine** are found in the **Total SO2**, **Sulphates** and the **Alcohol** level. We will need topay attention to those values.

On the other hand, the **RandomForest** method is strongly **biased towards** the **faulty classification**. However, the model is pretty good at predicting the good wines, and this is exacly what we are looking for. Simulation results show that the parameters that characterize a good wine are as follows:

* **Alcohol**: 
* **Total SO2**: 
* **Sulphates**: 
* **Residual Sugar**: 
* **Density**: 
* **Volatile acidity**: 

As it can be seen, both studies give similar results in terms of parameters. However, the **RandomForest** is not able to tell us if we need to increase or not these values to make a better wine. 

And now comes the funny part, **Do these parameters make sense? They do!!!!!**

Total acidity in wine is known as titratable acidity, and is the sum of the fixed and volatile acids. Total acidity directly effects the color and flavor of wine and, depending on the style of the wine, is sought in a perfect balance with the sweet and bitter sensation of other components. The regression says that for a better taste the acidity of the wine has to be composed by a higher amount of fixed than volatile acids, that means a stronger sweet taste. This is aligned with what the regression states about residual sugar and chlorides, a wine has better quality with a higher proportion of sugar and a low proportion of salt.

Sulphates are a preservative that's widely used in winemaking (and most food industries) for its antioxidant and antibacterial properties and they an important role in preventing oxidization and maintaining a wine's freshness. But in high amounts, sulphates such as S02 can have a disgusting smell and taste, that is why the regression says that a certain increase of sulphates are good but also that an increase of S02 reduces the quality of the wine.

In wine, alcohol and density are negative correlated. While water has a density of 1 gram per cubic centimeter, alcohol has a density of about 0.79 g/cc, so the more alcohol vs other liquids a wine contains should decrease the overall density of the wine. This is aligned as well with the regression, showing that a better quality is associated with more alcohol, causing less density.

![Good wine :)](Images\Wine happyness.jpg)

<font size="10">You cannot buy happiness, ... but you can buy a good wine and that is a kind of the same thing.</font>
